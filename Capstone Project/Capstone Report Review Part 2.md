# Capstone Report Review

Review date: 15 November 2019

<br>

### Meets Specifications 

![:trophy:](https://review.udacity.com/assets/images/emojis/trophy.png) Great job completing your capstone project! 

And if this is your final step to complete the nanodegree program, congratulations and best of luck with whatever [projects](https://www.analyticsvidhya.com/blog/2018/12/best-data-science-machine-learning-projects-github/) you decide to work on next! ![:sunglasses:](https://review.udacity.com/assets/images/emojis/sunglasses.png)

<br>

### Definition 

*Student provides a high-level overview of the project in layman’s terms. Background information such as the problem domain, the project origin, and related data sets or input data is given.*

OK

*The problem which needs to be solved is clearly defined. A strategy for solving the problem, including discussion of the expected solution, has been made.*

OK

*Metrics used to measure the performance of a model or result are clearly defined. Metrics are justified based on the characteristics of the problem.*

OK

Nice work describing the accuracy metric used. 

For more info, you can also see this post on [classification metrics](https://www.machinelearningplus.com/machine-learning/evaluation-metrics-classification-models-r/).

<br>

### Analysis 

*If a dataset is present, features and calculated statistics relevant to the problem have been reported and discussed, along with a sampling of the data. In lieu of a dataset, a thorough description of the input space or input data has been made. Abnormalities or characteristics of the data or input that need to be addressed have been identified.*

OK

Excellent job discussing the dataset and including the spectrogram [visualization](https://www.anaconda.com/blog/developer-blog/python-data-visualization-2018-why-so-many-libraries/) to help readers understand the audio data. ![:sunglasses:](https://review.udacity.com/assets/images/emojis/sunglasses.png)

For future [dataviz](https://serialmentor.com/dataviz/) ideas, you can also look into tools like [Altair](https://towardsdatascience.com/consistently-beautiful-visualizations-with-altair-themes-c7f9f889602), [Bokeh](https://bokeh.github.io/blog/2018/10/24/release-1-0-0/), [chartify](https://github.com/spotify/chartify). 

> [![altair](https://cdn-images-1.medium.com/max/1280/1*Awdq_0cscDJmULHzR42V2A.png)](https://cdn-images-1.medium.com/max/1280/1*Awdq_0cscDJmULHzR42V2A.png)[[altair](https://altair-viz.github.io/gallery/index.html)]

<br>

*A visualization has been provided that summarizes or extracts a relevant characteristic or feature about the dataset or input data with thorough discussion. Visual cues are clearly defined.*

OK

*Algorithms and techniques used in the project are thoroughly discussed and properly justified based on the characteristics of the problem.*

OK 

If interested in more resources on [deep learning](https://towardsdatascience.com/7-practical-deep-learning-tips-97a9f514100e) for computer vision, you can also go through [this collection of links](https://github.com/guillaume-chevalier/Awesome-Deep-Learning-Resources).

*Student clearly defines a benchmark result or threshold for comparing performances of solutions obtained.*

OK

### Methodology 

*All preprocessing steps have been clearly documented. Abnormalities or characteristics of the data or input that needed to be addressed have been corrected. If no data preprocessing is necessary, it has been clearly justified.*

OK

*The process for which metrics, algorithms, and techniques were implemented with the given datasets or input data has been thoroughly documented. Complications that occurred during the coding process are discussed.*

OK

*The process of improving upon the algorithms and techniques used is clearly documented. Both the initial and final solutions are reported, along with intermediate solutions, if necessary.*

OK

<br>

### Results 

*The final model’s qualities—such as parameters—are evaluated in detail. Some type of analysis is used to validate the robustness of the model’s solution.*

OK

Good job [evaluating](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html) your final model results, and [examining](https://towardsdatascience.com/interpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2) the robustness of the solution. ![:sunglasses:](https://review.udacity.com/assets/images/emojis/sunglasses.png)

For ideas on visualizations that can help with [model interpretability](https://github.com/lopusz/awesome-interpretable-machine-learning), I also recommend checking out techniques such as [LIME](https://github.com/marcotcr/lime) (Local Interpretable Model-agnostic Explanations) or [SHAP](https://github.com/slundberg/shap) (SHapley Additive exPlanations). 

> [![SHAP-image](https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/strawberry_example.png)](https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/strawberry_example.png)



*The final results are compared to the benchmark result or threshold with some type of statistical analysis. Justification is made as to whether the final model and solution is significant enough to have adequately solved the problem.*

OK

---

